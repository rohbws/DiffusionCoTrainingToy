{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    image_size = 128  # the generated image resolution\n",
    "    train_batch_size = 256\n",
    "    eval_batch_size = 256  # how many images to sample during evaluation\n",
    "    num_epochs = 500\n",
    "    patience = 20  # early stopping patience\n",
    "    gradient_accumulation_steps = 1\n",
    "    learning_rate = 1e-4\n",
    "    lr_warmup_steps = 500\n",
    "    save_image_epochs = 10\n",
    "    save_model_epochs = 30\n",
    "    output_dir = \"test-model-1\"  # the model name locally and on the HF Hub\n",
    "\n",
    "\n",
    "\n",
    "config = TrainingConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, ConcatDataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "class GaussianDataset(Dataset):\n",
    "    def __init__(self, num_samples, input_dim, cov, samples_per_obs):\n",
    "        self.num_samples = num_samples\n",
    "        self.input_dim = input_dim\n",
    "        self.cov = cov\n",
    "\n",
    "        self.o = []\n",
    "        self.a = []\n",
    "\n",
    "        if (samples_per_obs * num_samples) >= 1000000:\n",
    "            # Generate input data (o)\n",
    "            for _ in tqdm(range(num_samples)):\n",
    "                obs = np.random.normal(size=(input_dim))\n",
    "\n",
    "                for _ in range(samples_per_obs):\n",
    "\n",
    "                    # Generate output data (a) using the Gaussian distribution\n",
    "                    act = np.random.multivariate_normal(mean=obs, cov=self.cov)\n",
    "\n",
    "                    self.o.append(obs)\n",
    "                    self.a.append(act)\n",
    "        else:\n",
    "            # Generate input data (o)\n",
    "            for _ in range(num_samples):\n",
    "                obs = np.random.normal(size=(input_dim))\n",
    "\n",
    "                for _ in range(samples_per_obs):\n",
    "\n",
    "                    # Generate output data (a) using the Gaussian distribution\n",
    "                    act = np.random.multivariate_normal(mean=obs, cov=self.cov)\n",
    "\n",
    "                    self.o.append(obs)\n",
    "                    self.a.append(act)\n",
    "\n",
    "        print(f\"Generated {len(self.o)} samples\")\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.o)\n",
    "    \n",
    "    def get_observations(self):\n",
    "        return self.o\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        o = torch.tensor(self.o[idx], dtype=torch.float32)\n",
    "        a = torch.tensor(self.a[idx], dtype=torch.float32)\n",
    "        return o, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_57086/1427197143.py:35: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
      "  act = np.random.multivariate_normal(mean=obs, cov=self.cov)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 100000 samples\n",
      "Generated 1000 samples\n",
      "Generated 10000 samples\n",
      "torch.Size([256, 2]) torch.Size([256, 2])\n"
     ]
    }
   ],
   "source": [
    "# Set parameters\n",
    "real_samples = 100\n",
    "sim_saples = 10000\n",
    "action_dim = 2\n",
    "cov = np.array([[0.1, 0.05], [0.05, 0.1]])\n",
    "sim_cov = cov + np.random.normal(0, 0.01, cov.shape)\n",
    "samples_per_obs = 10\n",
    "\n",
    "# Create dataset\n",
    "real_dataset = GaussianDataset(real_samples, action_dim, cov, samples_per_obs)\n",
    "\n",
    "sim_dataset = GaussianDataset(sim_saples, action_dim, sim_cov, samples_per_obs)\n",
    "\n",
    "val_dataset = GaussianDataset(100, action_dim, cov, samples_per_obs)\n",
    "\n",
    "test_dataset = GaussianDataset(1000, action_dim, cov, samples_per_obs)\n",
    "\n",
    "# Concatenate datasets\n",
    "dataset = ConcatDataset([real_dataset, sim_dataset])\n",
    "\n",
    "# Create data loader\n",
    "batch_size = config.train_batch_size\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_data_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for step, batch in enumerate(data_loader):\n",
    "    o, a = batch\n",
    "    print(o.shape, a.shape)\n",
    "    break\n",
    "\n",
    "# Construct validation dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rbosworth/DiffusionCoTrainingToy/diffusion_toy/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassConditionedMLP(\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=5, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=64, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from diffusers import UNet2DModel\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ClassConditionedMLP(nn.Module):\n",
    "    def __init__(self, action_dim=2, class_emb_size=2, hidden_dim=64, num_layers=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Input dimension: (action_dim + class_emb_size) + 1 (timestep)\n",
    "        input_dim = action_dim * 1 + class_emb_size * 1 + 1\n",
    "        output_dim = action_dim * 1  # Matches the output shape of the UNet\n",
    "        \n",
    "        # Define a simple MLP architecture\n",
    "        layers = [nn.Linear(input_dim, hidden_dim), nn.ReLU()]\n",
    "        for _ in range(num_layers - 1):\n",
    "            layers.extend([nn.Linear(hidden_dim, hidden_dim), nn.ReLU()])\n",
    "        layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "        \n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, obs, act, t):\n",
    "        # Flatten input dimensions and prepare input for MLP\n",
    "        act = torch.tensor(act).unsqueeze(1).unsqueeze(-1)  # Convert to tensor and shape (bs, 1, 2, 1)\n",
    "        bs, ch, w, h = act.shape\n",
    "        act_flat = act.view(bs, -1)  # Flatten to (bs, action_dim)\n",
    "        \n",
    "        class_cond = torch.tensor(obs)  # Convert to tensor (bs, class_emb_size)\n",
    "        t = torch.tensor(t).unsqueeze(-1)  # Convert timestep to tensor (bs, 1)\n",
    "        \n",
    "        # Concatenate action, class conditioning, and timestep\n",
    "        mlp_input = torch.cat([act_flat, class_cond, t], dim=1)  # Shape: (bs, input_dim)\n",
    "        \n",
    "        # Pass through MLP\n",
    "        output = self.mlp(mlp_input)  # Shape: (bs, output_dim)\n",
    "        \n",
    "        # Reshape to match UNet's output: (bs, 1, action_dim, 1)\n",
    "        return output.view(bs, 1, w, h)\n",
    "\n",
    "\n",
    "class ClassConditionedUnet(nn.Module):\n",
    "    def __init__(self, action_dim=2, class_emb_size=2):\n",
    "        super().__init__()\n",
    "\n",
    "        # Self.model is an unconditional UNet with extra input channels to accept the conditioning information (the class embedding)\n",
    "        self.model = UNet2DModel(\n",
    "            sample_size=(action_dim, 1),  # the target image resolution\n",
    "            in_channels=1 + class_emb_size,  # Additional input channels for class cond.\n",
    "            out_channels=1,  # the number of output channels\n",
    "            block_out_channels=(32,),\n",
    "            down_block_types=(\n",
    "                \"DownBlock2D\",  # a regular ResNet downsampling block\n",
    "            ),\n",
    "            up_block_types=(\n",
    "                \"UpBlock2D\",  # a regular ResNet upsampling block\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    # Our forward method now takes the class labels as an additional argument\n",
    "    def forward(self, obs, act, t):\n",
    "        act = torch.Tensor(act).unsqueeze(1).unsqueeze(-1)  # Convert to tensor and move to device\n",
    "        # Shape of x:\n",
    "        bs, ch, w, h = act.shape\n",
    "\n",
    "        # class conditioning in right shape to add as additional input channels\n",
    "        class_cond = torch.Tensor(obs) # Convert to tensor and move to device\n",
    "        class_cond = class_cond.view(bs, class_cond.shape[1], 1, 1).expand(bs, class_cond.shape[1], w, h)\n",
    "        # x is shape (bs, 1, 2, 1) and class_cond is now (bs, 2, 2, 1)\n",
    "\n",
    "        # Net input is now x and class cond concatenated together along dimension 1\n",
    "        net_input = torch.cat((act, class_cond), 1)  # (bs, 3, 2, 1)\n",
    "\n",
    "        # Feed this to the UNet alongside the timestep and return the prediction\n",
    "        return self.model(net_input, t).sample  # (bs, 1, 2, 1)\n",
    "\n",
    "# Initialize the MLP with the desired action dimension\n",
    "model = ClassConditionedMLP(action_dim=action_dim)\n",
    "\n",
    "# Print the MLP architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([5])\n",
      "Number of weights: 8834\n"
     ]
    }
   ],
   "source": [
    "o, a = real_dataset[0]\n",
    "sample_inp = torch.cat([o, a, torch.tensor([0])])\n",
    "print(\"Input shape:\", sample_inp.shape)\n",
    "\n",
    "num_weights = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of weights: {num_weights}\")\n",
    "\n",
    "# Forward pass\n",
    "#output = model(o, a, torch.tensor([0]))\n",
    "#print(\"Output shape:\", output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noisy image shape: torch.Size([2])\n",
      "Noisy image: tensor([0.2856, 1.6231])\n"
     ]
    }
   ],
   "source": [
    "from diffusers import DDPMScheduler\n",
    "import torch.nn.functional as F\n",
    "\n",
    "noise_scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
    "noise = torch.randn(a.shape)\n",
    "timesteps = torch.LongTensor([50])\n",
    "noisy_image = noise_scheduler.add_noise(a, noise, timesteps)\n",
    "\n",
    "observation = torch.tensor([0.0, 0.0])\n",
    "\n",
    "print(\"Noisy image shape:\", noisy_image.shape)\n",
    "print(\"Noisy image:\", noisy_image)\n",
    "\n",
    "\n",
    "#noise_pred = model(noisy_image, observation, timesteps)\n",
    "#loss = F.mse_loss(noise_pred, noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=config.lr_warmup_steps,\n",
    "    num_training_steps=(len(data_loader) * config.num_epochs),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from huggingface_hub import create_repo, upload_folder\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "import os\n",
    "from diffusers import DDPMPipeline\n",
    "from diffusers.utils import make_image_grid\n",
    "import os\n",
    "import wandb\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "wandb.init(project=\"diffusion-toy\", entity=\"rohanb27-csail\")\n",
    "\n",
    "\n",
    "def train_loop(config, model, noise_scheduler, optimizer, train_dataloader, val_dataloader, lr_scheduler):\n",
    "    # Initialize accelerator and tensorboard logging\n",
    "    accelerator = Accelerator(\n",
    "        gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "        log_with=\"wandb\",\n",
    "        project_dir=os.path.join(config.output_dir, \"logs\"),\n",
    "    )\n",
    "    if accelerator.is_main_process:\n",
    "        if config.output_dir is not None:\n",
    "            os.makedirs(config.output_dir, exist_ok=True)\n",
    "\n",
    "    # Prepare everything\n",
    "    # There is no specific order to remember, you just need to unpack the\n",
    "    # objects in the same order you gave them to the prepare method.\n",
    "    model, optimizer, train_dataloader, val_dataloader, lr_scheduler = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, val_dataloader, lr_scheduler\n",
    "    )\n",
    "\n",
    "    global_step = 0\n",
    "\n",
    "    epochs_since_improvement = 0\n",
    "\n",
    "    prev_val_loss = float(\"inf\")\n",
    "\n",
    "    # Now you train the model\n",
    "    for epoch in range(config.num_epochs):\n",
    "        progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process)\n",
    "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            # Unpack the batch\n",
    "            obs, act = batch\n",
    "            # Concatenate observation and action\n",
    "            inputs = torch.cat((obs, act), dim=1)\n",
    "            bs = inputs.shape[0]\n",
    "\n",
    "            # Sample noise to add to the actions\n",
    "            noise = torch.randn(act.shape, device=act.device)\n",
    "\n",
    "            # Sample a random timestep for each action\n",
    "            timesteps = torch.randint(\n",
    "                0, noise_scheduler.config.num_train_timesteps, (bs,), device=act.device,\n",
    "                dtype=torch.int64\n",
    "            )\n",
    "\n",
    "            # Add noise to the clean actions according to the noise magnitude at each timestep\n",
    "            # (this is the forward diffusion process)\n",
    "            noisy_actions = noise_scheduler.add_noise(act, noise, timesteps)\n",
    "            \n",
    "            with accelerator.accumulate(model):\n",
    "                # Predict the noise residual\n",
    "                noise_pred = model(obs, noisy_actions, timesteps).squeeze()\n",
    "\n",
    "                loss = F.mse_loss(noise_pred, noise)\n",
    "                accelerator.backward(loss)\n",
    "\n",
    "                if accelerator.sync_gradients:\n",
    "                    accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            progress_bar.update(1)\n",
    "            logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0], \"step\": global_step}\n",
    "            progress_bar.set_postfix(**logs)\n",
    "            accelerator.log(logs, step=global_step)\n",
    "            global_step += 1\n",
    "\n",
    "        # Validate after each epoch\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                obs, act = batch\n",
    "                inputs = torch.cat((obs, act), dim=1)\n",
    "                bs = inputs.shape[0]\n",
    "\n",
    "                noise = torch.randn(act.shape, device=act.device)\n",
    "                timesteps = torch.randint(\n",
    "                    0, noise_scheduler.config.num_train_timesteps, (bs,), device=act.device,\n",
    "                    dtype=torch.int64\n",
    "                )\n",
    "                noisy_actions = noise_scheduler.add_noise(act, noise, timesteps)\n",
    "                noise_pred = model(obs, noisy_actions, timesteps).squeeze()\n",
    "                loss = F.mse_loss(noise_pred, noise)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_dataloader)\n",
    "        accelerator.log({\"val_loss\": val_loss}, step=global_step)\n",
    "\n",
    "        if val_loss < prev_val_loss:\n",
    "            prev_val_loss = val_loss\n",
    "            epochs_since_improvement = 0\n",
    "        else:\n",
    "            epochs_since_improvement += 1\n",
    "\n",
    "        if epochs_since_improvement >= config.patience:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "        # Save the model after each epoch\n",
    "        if accelerator.is_main_process and epoch % config.save_model_epochs == 0:\n",
    "            ...\n",
    "            #pipeline = DDPMPipeline(unet=accelerator.unwrap_model(model.model), scheduler=noise_scheduler)\n",
    "            #pipeline.save_pretrained(config.output_dir)\n",
    "\n",
    "        model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on one GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/395 [00:00<?, ?it/s]/tmp/ipykernel_57086/1307585270.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  act = torch.tensor(act).unsqueeze(1).unsqueeze(-1)  # Convert to tensor and shape (bs, 1, 2, 1)\n",
      "/tmp/ipykernel_57086/1307585270.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  class_cond = torch.tensor(obs)  # Convert to tensor (bs, class_emb_size)\n",
      "/tmp/ipykernel_57086/1307585270.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).unsqueeze(-1)  # Convert timestep to tensor (bs, 1)\n",
      "Epoch 0: 100%|██████████| 395/395 [00:03<00:00, 126.11it/s, loss=1.08, lr=7.9e-5, step=394]  \n",
      "Epoch 1: 100%|██████████| 395/395 [00:03<00:00, 114.57it/s, loss=0.978, lr=0.0001, step=789]\n",
      "Epoch 2: 100%|██████████| 395/395 [00:02<00:00, 140.57it/s, loss=0.863, lr=0.0001, step=1184]\n",
      "Epoch 3: 100%|██████████| 395/395 [00:03<00:00, 117.45it/s, loss=0.797, lr=0.0001, step=1579]\n",
      "Epoch 4: 100%|██████████| 395/395 [00:02<00:00, 142.57it/s, loss=0.549, lr=0.0001, step=1974]\n",
      "Epoch 5: 100%|██████████| 395/395 [00:03<00:00, 120.25it/s, loss=0.443, lr=0.0001, step=2369]\n",
      "Epoch 6: 100%|██████████| 395/395 [00:02<00:00, 136.22it/s, loss=0.39, lr=0.0001, step=2764] \n",
      "Epoch 7: 100%|██████████| 395/395 [00:03<00:00, 118.91it/s, loss=0.27, lr=0.0001, step=3159]\n",
      "Epoch 8: 100%|██████████| 395/395 [00:03<00:00, 128.57it/s, loss=0.346, lr=9.99e-5, step=3554]\n",
      "Epoch 9: 100%|██████████| 395/395 [00:03<00:00, 118.55it/s, loss=0.362, lr=9.99e-5, step=3949]\n",
      "Epoch 10: 100%|██████████| 395/395 [00:02<00:00, 143.19it/s, loss=0.27, lr=9.99e-5, step=4344] \n",
      "Epoch 11: 100%|██████████| 395/395 [00:03<00:00, 115.63it/s, loss=0.284, lr=9.99e-5, step=4739]\n",
      "Epoch 12: 100%|██████████| 395/395 [00:02<00:00, 143.03it/s, loss=0.17, lr=9.99e-5, step=5134] \n",
      "Epoch 13: 100%|██████████| 395/395 [00:03<00:00, 116.67it/s, loss=0.221, lr=9.98e-5, step=5529]\n",
      "Epoch 14: 100%|██████████| 395/395 [00:02<00:00, 141.58it/s, loss=0.319, lr=9.98e-5, step=5924]\n",
      "Epoch 15: 100%|██████████| 395/395 [00:03<00:00, 114.97it/s, loss=0.269, lr=9.98e-5, step=6319]\n",
      "Epoch 16: 100%|██████████| 395/395 [00:02<00:00, 140.19it/s, loss=0.329, lr=9.98e-5, step=6714]\n",
      "Epoch 17: 100%|██████████| 395/395 [00:03<00:00, 119.82it/s, loss=0.282, lr=9.97e-5, step=7109]\n",
      "Epoch 18: 100%|██████████| 395/395 [00:02<00:00, 133.24it/s, loss=0.24, lr=9.97e-5, step=7504] \n",
      "Epoch 19: 100%|██████████| 395/395 [00:03<00:00, 121.65it/s, loss=0.195, lr=9.97e-5, step=7899]\n",
      "Epoch 20: 100%|██████████| 395/395 [00:02<00:00, 132.50it/s, loss=0.257, lr=9.96e-5, step=8294]\n",
      "Epoch 21: 100%|██████████| 395/395 [00:03<00:00, 120.48it/s, loss=0.266, lr=9.96e-5, step=8689]\n",
      "Epoch 22: 100%|██████████| 395/395 [00:02<00:00, 140.84it/s, loss=0.299, lr=9.95e-5, step=9084]\n",
      "Epoch 23: 100%|██████████| 395/395 [00:03<00:00, 111.67it/s, loss=0.254, lr=9.95e-5, step=9479]\n",
      "Epoch 24: 100%|██████████| 395/395 [00:02<00:00, 139.00it/s, loss=0.284, lr=9.94e-5, step=9874]\n",
      "Epoch 25: 100%|██████████| 395/395 [00:03<00:00, 115.91it/s, loss=0.29, lr=9.94e-5, step=10269]\n",
      "Epoch 26: 100%|██████████| 395/395 [00:02<00:00, 151.72it/s, loss=0.303, lr=9.93e-5, step=10664]\n",
      "Epoch 27: 100%|██████████| 395/395 [00:03<00:00, 120.01it/s, loss=0.249, lr=9.93e-5, step=11059]\n",
      "Epoch 28: 100%|██████████| 395/395 [00:02<00:00, 141.19it/s, loss=0.266, lr=9.92e-5, step=11454]\n",
      "Epoch 29: 100%|██████████| 395/395 [00:03<00:00, 124.28it/s, loss=0.238, lr=9.92e-5, step=11849]\n",
      "Epoch 30: 100%|██████████| 395/395 [00:02<00:00, 141.72it/s, loss=0.275, lr=9.91e-5, step=12244]\n",
      "Epoch 31: 100%|██████████| 395/395 [00:03<00:00, 125.65it/s, loss=0.342, lr=9.91e-5, step=12639]\n",
      "Epoch 32: 100%|██████████| 395/395 [00:02<00:00, 151.65it/s, loss=0.305, lr=9.9e-5, step=13034] \n",
      "Epoch 33: 100%|██████████| 395/395 [00:03<00:00, 121.13it/s, loss=0.257, lr=9.89e-5, step=13429]\n",
      "Epoch 34: 100%|██████████| 395/395 [00:02<00:00, 142.25it/s, loss=0.288, lr=9.89e-5, step=13824]\n",
      "Epoch 35: 100%|██████████| 395/395 [00:03<00:00, 117.81it/s, loss=0.252, lr=9.88e-5, step=14219]\n",
      "Epoch 36: 100%|██████████| 395/395 [00:02<00:00, 147.52it/s, loss=0.221, lr=9.87e-5, step=14614]\n",
      "Epoch 37: 100%|██████████| 395/395 [00:03<00:00, 117.44it/s, loss=0.238, lr=9.87e-5, step=15009]\n",
      "Epoch 38:  64%|██████▍   | 254/395 [00:01<00:00, 145.94it/s, loss=0.259, lr=9.86e-5, step=15263]"
     ]
    }
   ],
   "source": [
    "from accelerate import notebook_launcher\n",
    "args = (config, model, noise_scheduler, optimizer, data_loader, val_data_loader, lr_scheduler)\n",
    "notebook_launcher(train_loop, args, num_processes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 2]) torch.Size([256, 2])\n"
     ]
    }
   ],
   "source": [
    "for step, batch in enumerate(val_data_loader):\n",
    "    o, a = batch\n",
    "    print(o.shape, a.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [00:04, 230.38it/s]\n"
     ]
    }
   ],
   "source": [
    "# @markdown Sampling some different digits:\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Prepare random x to start from, plus some desired labels y\n",
    "obs, act_desired = sim_dataset[0:2]\n",
    "\n",
    "act = torch.randn(2, 2)\n",
    "act = act.to(device)\n",
    "obs = obs.to(device)\n",
    "\n",
    "        \n",
    "# Sampling loop\n",
    "for i, t in tqdm(enumerate(noise_scheduler.timesteps)):\n",
    "    # Get model pred\n",
    "    with torch.no_grad():\n",
    "        residual = model(obs, act, t).squeeze() # Again, note that we pass in our labels y\n",
    "\n",
    "    #print(noise_scheduler.step(residual, t, act))\n",
    "\n",
    "    # Update sample with step\n",
    "    act = noise_scheduler.step(residual, t, act).prev_sample\n",
    "\n",
    "\n",
    "# Show the results\n",
    "#fig, ax = plt.subplots(1, 1, figsize=(12, 12))\n",
    "#ax.imshow(torchvision.utils.make_grid(x.detach().cpu().clip(-1, 1), nrow=8)[0], cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3133,  0.2239],\n",
      "        [ 0.8985,  0.6241]])\n",
      "tensor([[ 0.6501,  0.2818],\n",
      "        [-0.0813, -0.4879]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(act_desired)\n",
    "print(act)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion_toy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
